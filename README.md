This repository contains the source code for our paper [Bidirectional Transformer Reranker for Grammatical Error Correction](https://arxiv.org/abs/2305.13000).

## Getting Started
### Requirements
* [PyTorch](http://pytorch.org/) version == 1.10.1
* Python version >= 3.7

### Clone this repository 
```sh
git clone https://github.com/zhangying9128/BTR.git
```

### Install transformers ([Huggingface](https://github.com/huggingface/transformers/tree/v4.11.3)) from our repository
Please use our modified transformers.
```sh
cd transformers
pip install --editable .
cd ..
```

### Install [fairseq](https://github.com/facebookresearch/fairseq/tree/v0.10.0) from out repository
Please use our modified fairseq.
```sh
cd fairseq/
pip install --editable .
cd ..
```

## Pre-training the BTR
To reproduce our experimental results, you can follow our steps to pre-train it by yourselves.
Or directly downloading our [pre-trained BTR]() to the corresponding folders in [checkpoints]().

### Download Realnewslike dataset
As we mentioned in our paper, we utilized the [Realnewslike](https://huggingface.co/datasets/allenai/c4/tree/main/realnewslike) dataset for pretraining the BTR, which is a subset of the [C4](https://huggingface.co/datasets/allenai/c4) dataset.
Please use the following command to download and preprocess the Realnewslike dataset.
```sh
bash commands/preprocess_realnewslike.sh
```

### Pre-training the BTR with Realnewslike dataset
Before pre-training, please download our [fine-tuned T5GEC]() to the corresponding folders in [checkpoints]() to speed-up pre-training.
And then use the following command to pre-train the BTR. 
You need to modify commands CUDA_VISIBLE_DEVICES and --distributed-world-size according to your environment.
Batch size = MAX_TOKENS * UPDATE_FREQ * distributed_world_size.
Our experiment here used two gpus, therefore we set --distributed-world-size=2.
```sh
SAVE_PATH=checkpoints/pre-trained-BTR/
mkdir -p $SAVE_PATH
bash commands/pretrain_BTR.sh
```

## Fine-tuning the BTR
After getting pre-trained BTR, you can use the following steps to fine-tune it by yourselves.
Or directly download our [fine-tuned BTR]() to the corresponding folders in [checkpoints]().
As we mentioned in our paper, we run 4 trials with random seeds. You can use the following fined-tuned BTR models to reproduce our results.

|Trial 1| Trial 2|Trial 3|Trial 4|
|---|---|---|---|
[1]()| [2]() | [3]() | [4]() | 

### Download dataset
Please follow our [suggestions](https://github.com/zhangying9128/BTR/tree/main/datasets) to download grammatical error correction datasets, and put their corresponding .src and .tgt files to the corresponding folders in [datasets](https://github.com/zhangying9128/BTR/tree/main/datasets).
Specifically, as we mentioned in our paper, we utilize the cleaned [CoNLL-13](https://github.com/zhangying9128/BTR/blob/main/data/DEV_CoNLL13_cleaned/official-preprocessed-cleanpunc.m2) and [CoNLL-14](https://github.com/zhangying9128/BTR/blob/main/data/TEST_CoNLL14_cleaned/official-2014.cleanpuncsplit.m2) dataset, please check the corresponding m2 files for more details.

### Process CoNLL-13 dataset
Here we give an example of processing CoNLL-13 dataset and constructing a data bin from processed cLang8, CoNLL-13, and CoNLL-14 datasets. 
Please use the following commads to process data for using fairseq.
```sh
bash commands/preprocess_gec_datasets.sh
```

### Process candidates for CoNLL-13 dataset
You can use your own candidate files or use our provided candidates (generated by the T5GEC) that used in our paper, please check the folders in [data](https://github.com/zhangying9128/BTR/tree/main/data) and this [link]().
Here we give an example of processing candidates for CoNLL-13 dataset. 
Please use the following commads to do tokenization for candidates.
```sh
bash commands/process_candidates.sh
```

### Fine-tuning the BTR with cLang8 dataset
Before finetuning, please copy the pre-trained BTR to the SAVE_PATH for loading pre-trained parameters.
And then use the following command to fine-tune the BTR.
```sh
SAVE_PATH=checkpoints/fine-tuned-BTR/
bash commands/finetune_BTR.sh
```


## Reranking candidates with the BTR
Please use the following command to rerank candidates with the BTR.
This command will help to generate and save the corresponding score (formula 9 in our paper) for each candidate.
```sh
bash commands/rerank.sh
```

## Citation:
Please cite as:
```bibtex

```